# ██████╗  ██████╗ █████╗
# ██╔══██╗██╔════╝██╔══██╗
# ██║  ██║██║     ███████║
# ██║  ██║██║     ██╔══██║
# ██████╔╝╚██████╗██║  ██║
# ╚═════╝  ╚═════╝╚═╝  ╚═╝
# DEPARTAMENTO DE ENGENHARIA DE COMPUTACAO E AUTOMACAO
# UNIVERSIDADE FEDERAL DO RIO GRANDE DO NORTE, NATAL/RN
#
# (C) 2022-2025 CARLOS M D VIEGAS
# https://github.com/cmdviegas

### Description:
# This is a docker-compose file that creates a stack of nodes running Apache Hadoop 3.4.0 and Spark 3.5.3.

### How it works:
# It initializes 'spark-master' container with hadoop and spark services and multiple 'worker-X' containers as worker nodes (according to the number of replicas). 'spark-master' starts hadoop and spark services and then creates a cluster by connecting to each 'worker-X'. There is an .env file that defines some environment variables that should be edited by the user.

services:
  spark-master:
    container_name: ${MASTER_HOSTNAME}
    hostname: ${MASTER_HOSTNAME}
    image: sparkcluster/${IMAGE_NAME}
    build:
      context: .
      dockerfile: Dockerfile
      args:
        USER: ${USERNAME} # USERNAME (change at .env file)
        PASS: ${PASSWORD} # USER PASSWORD (change at .env file)
        SPARK_VER: ${SPARK_VERSION}
        HADOOP_VER: ${HADOOP_VERSION}
    tty: true
    restart: no
    networks:
      spark_network:
        ipv4_address: ${IP_MASTER}
    ports:
      - "9870:9870/tcp" # HDFS
      - "8088:8088/tcp" # YARN
      - "4040:4040/tcp" # SPARK HISTORY (DURING A VALID SPARKSESSION)
      - "18080:18080/tcp" # SPARK HISTORY SERVER
      - "2222:22/tcp" # SSH
    volumes: 
      - ./myfiles:/home/${USERNAME}/myfiles
      - .env:/home/${USERNAME}/.env
      - master:/home/${USERNAME}/
    entrypoint: ./bootstrap.sh
    command: MASTER
    healthcheck:
      test: bash -c 'ssh -q -o ConnectTimeout=1 ${USERNAME}@${MASTER_HOSTNAME} exit'
      start_period: 3s
      interval: 2s
      timeout: 3s
      retries: 3

  worker:
    image: sparkcluster/${IMAGE_NAME}
    deploy:
      mode: replicated
      replicas: ${REPLICAS}
    tty: true
    restart: on-failure:2
    networks:
      - spark_network
    depends_on:
      spark-master:
        condition: service_healthy
    entrypoint: ./bootstrap.sh
    command: WORKER
    volumes: 
      - ./myfiles:/home/${USERNAME}/myfiles
      - .env:/home/${USERNAME}/.env
      - /home/${USERNAME}/

networks:
  spark_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: ${IP_RANGE}

volumes:
  master:
