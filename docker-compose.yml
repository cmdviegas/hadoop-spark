# ██████╗  ██████╗ █████╗
# ██╔══██╗██╔════╝██╔══██╗
# ██║  ██║██║     ███████║
# ██║  ██║██║     ██╔══██║
# ██████╔╝╚██████╗██║  ██║
# ╚═════╝  ╚═════╝╚═╝  ╚═╝
# DEPARTAMENTO DE ENGENHARIA DE COMPUTACAO E AUTOMACAO
# UNIVERSIDADE FEDERAL DO RIO GRANDE DO NORTE, NATAL/RN
#
# (C) 2022-2025 CARLOS M D VIEGAS
# https://github.com/cmdviegas

# Description: This file sets up a multi-node Hadoop and Spark cluster.

# ⚠️ This file is automatically generated by the init.sh script. 
# You can edit it manually, but any changes will be overwritten the next time init.sh runs.

# If you want to add your own services, use the docker-compose.aux.yml file instead. 
# Then start the cluster with: docker compose -f docker-compose.yml -f docker-compose.aux.yml up

name: ${STACK_NAME}

x-spark-common:
  &spark-common
  image: ${IMAGE_NAME}
  tty: true
  restart: on-failure
  entrypoint: ["bash", "bootstrap.sh"]
  networks:
    - ${STACK_NAME}_network
  secrets:
    - user_password
  environment:
    - MY_SECRETS_FILE=/run/secrets/user_password
    - NUM_WORKER_NODES=${NUM_WORKER_NODES}
    - STACK_NAME=${STACK_NAME}

networks:
  spark_network:
    name: ${STACK_NAME}_network
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.31.0.0/24

volumes:
  master:
    name: ${STACK_NAME}_master_volume
    driver: local

secrets:
  user_password:
    file: .password

services:
  init:
    image: alpine:3.22.0
    container_name: ${STACK_NAME}-init
    profiles: [init]
    tty: true
    restart: no
    working_dir: /workspace
    networks:
      - ${STACK_NAME}_network
    volumes:
      - ./:/workspace
    environment:
      - NUM_WORKER_NODES=${NUM_WORKER_NODES}
      - DOCKER_COMPOSE_RUN=true
      - DOWNLOAD_HADOOP_SPARK=true
    entrypoint: ["sh", "script_files/init.sh"]

  worker-1:
    <<: *spark-common
    container_name: ${STACK_NAME}-worker-1
    hostname: ${STACK_NAME}-worker-1
    volumes:
      - ./myfiles:/home/myuser/myfiles
      - ./config_files/hadoop/core-site.xml:/home/myuser/hadoop/etc/hadoop/core-site.xml
      - ./config_files/hadoop/hadoop-env.sh:/home/myuser/hadoop/etc/hadoop/hadoop-env.sh
      - ./config_files/hadoop/hdfs-site.xml:/home/myuser/hadoop/etc/hadoop/hdfs-site.xml
      - ./config_files/hadoop/mapred-site.xml:/home/myuser/hadoop/etc/hadoop/mapred-site.xml
      - ./config_files/hadoop/yarn-env.sh:/home/myuser/hadoop/etc/hadoop/yarn-env.sh
      - ./config_files/hadoop/yarn-site.xml:/home/myuser/hadoop/etc/hadoop/yarn-site.xml
      - ./config_files/spark/spark-defaults.conf:/home/myuser/spark/conf/spark-defaults.conf
      - ./config_files/spark/spark-env.sh:/home/myuser/spark/conf/spark-env.sh
      - .env:/home/myuser/.env
    command: ["WORKER"]

  worker-2:
    <<: *spark-common
    container_name: ${STACK_NAME}-worker-2
    hostname: ${STACK_NAME}-worker-2
    volumes:
      - ./myfiles:/home/myuser/myfiles
      - ./config_files/hadoop/core-site.xml:/home/myuser/hadoop/etc/hadoop/core-site.xml
      - ./config_files/hadoop/hadoop-env.sh:/home/myuser/hadoop/etc/hadoop/hadoop-env.sh
      - ./config_files/hadoop/hdfs-site.xml:/home/myuser/hadoop/etc/hadoop/hdfs-site.xml
      - ./config_files/hadoop/mapred-site.xml:/home/myuser/hadoop/etc/hadoop/mapred-site.xml
      - ./config_files/hadoop/yarn-env.sh:/home/myuser/hadoop/etc/hadoop/yarn-env.sh
      - ./config_files/hadoop/yarn-site.xml:/home/myuser/hadoop/etc/hadoop/yarn-site.xml
      - ./config_files/spark/spark-defaults.conf:/home/myuser/spark/conf/spark-defaults.conf
      - ./config_files/spark/spark-env.sh:/home/myuser/spark/conf/spark-env.sh
      - .env:/home/myuser/.env
    command: ["WORKER"]

  master:
    <<: *spark-common
    container_name: ${STACK_NAME}-master
    hostname: ${STACK_NAME}-master
    build:
      context: .
      dockerfile: Dockerfile
      args:
        SPARK_VERSION: ${SPARK_VERSION}
        HADOOP_VERSION: ${HADOOP_VERSION}
        APT_MIRROR: ${APT_MIRROR}
    ports:
      - "9870:9870/tcp" # HDFS
      - "8088:8088/tcp" # YARN
      - "19888:19888/tcp" # Mapred Job History
      - "18080:18080/tcp" # Spark Job History
      - "15002:15002/tcp" # Spark Connect
      - "8888:8888/tcp" # Jupyter Lab
    volumes:
      - master:/home/myuser/
      - ./myfiles:/home/myuser/myfiles
      - ./config_files/hadoop/core-site.xml:/home/myuser/hadoop/etc/hadoop/core-site.xml
      - ./config_files/hadoop/hadoop-env.sh:/home/myuser/hadoop/etc/hadoop/hadoop-env.sh
      - ./config_files/hadoop/hdfs-site.xml:/home/myuser/hadoop/etc/hadoop/hdfs-site.xml
      - ./config_files/hadoop/mapred-site.xml:/home/myuser/hadoop/etc/hadoop/mapred-site.xml
      - ./config_files/hadoop/yarn-env.sh:/home/myuser/hadoop/etc/hadoop/yarn-env.sh
      - ./config_files/hadoop/yarn-site.xml:/home/myuser/hadoop/etc/hadoop/yarn-site.xml
      - ./config_files/spark/spark-defaults.conf:/home/myuser/spark/conf/spark-defaults.conf
      - ./config_files/spark/spark-env.sh:/home/myuser/spark/conf/spark-env.sh
      - .env:/home/myuser/.env
    command: ["MASTER"]
