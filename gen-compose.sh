#!/usr/bin/env bash
# ██████╗  ██████╗ █████╗
# ██╔══██╗██╔════╝██╔══██╗
# ██║  ██║██║     ███████║
# ██║  ██║██║     ██╔══██║
# ██████╔╝╚██████╗██║  ██║
# ╚═════╝  ╚═════╝╚═╝  ╚═╝
# DEPARTAMENTO DE ENGENHARIA DE COMPUTACAO E AUTOMACAO
# UNIVERSIDADE FEDERAL DO RIO GRANDE DO NORTE, NATAL/RN
#
# (C) 2022-2025 CARLOS M D VIEGAS
# https://github.com/cmdviegas
#
# Description: This script dynamically updates the docker-compose.yml file based on the number of worker nodes defined in the .env file.

# You can edit this file to suit your requirements.

# Do not execute this file directly. Instead, use: docker compose run --rm gen-compose

source /workspace/config_files/system/.bash_common

log_info() { printf "%b %s\n" "${INFO}" "$1"; }
log_warn() { printf "%b %s\n" "${WARN}" "$1"; }

if [ -z "$COMPOSE_CHECK" ]; then
  log_warn "This script must be executed using: ${YELLOW_COLOR}docker compose run --rm gen-compose${RESET_COLORS}"
  exit 1
fi

COMPOSE_FILE="docker-compose.yml"
NUM_WORKER_NODES=${NUM_WORKER_NODES}

cat > "$COMPOSE_FILE" << EOF
# ██████╗  ██████╗ █████╗
# ██╔══██╗██╔════╝██╔══██╗
# ██║  ██║██║     ███████║
# ██║  ██║██║     ██╔══██║
# ██████╔╝╚██████╗██║  ██║
# ╚═════╝  ╚═════╝╚═╝  ╚═╝
# DEPARTAMENTO DE ENGENHARIA DE COMPUTACAO E AUTOMACAO
# UNIVERSIDADE FEDERAL DO RIO GRANDE DO NORTE, NATAL/RN
#
# (C) 2022-2025 CARLOS M D VIEGAS
# https://github.com/cmdviegas

# Description: This file sets up a multi-node Hadoop and Spark cluster.

# ⚠️ This file is auto-generated by the gen-compose.sh script. It can be edited manually, but any changes will be overwritten if gen-compose.sh is run afterward.

name: \${STACK_NAME}

x-spark-common:
  &spark-common
  image: \${IMAGE_NAME}
  tty: true
  restart: on-failure
  entrypoint: ["bash", "bootstrap.sh"]
  networks:
    - \${STACK_NAME}_network
  secrets:
    - user_password
  environment:
    - MY_SECRETS_FILE=/run/secrets/user_password
    - NUM_WORKER_NODES=\${NUM_WORKER_NODES}
    - STACK_NAME=\${STACK_NAME}

networks:
  spark_network:
    name: \${STACK_NAME}_network
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.31.0.0/24

volumes:
  master:
    name: \${STACK_NAME}_master_volume
    driver: local

secrets:
  user_password:
    file: .password

services:
  gen-compose:
    image: ubuntu:24.04
    container_name: \${STACK_NAME}-init
    profiles: [gen-compose]
    working_dir: /workspace
    networks:
      - \${STACK_NAME}_network
    volumes:
      - ./:/workspace
    environment:
      - NUM_WORKER_NODES=\${NUM_WORKER_NODES}
      - COMPOSE_CHECK=true
    entrypoint: ["bash", "gen-compose.sh"]

EOF

if [ "$1" != "clear" ]; then

  for i in $(seq 1 $NUM_WORKER_NODES); do
    cat >> "$COMPOSE_FILE" << EOF
  worker-$i:
    <<: *spark-common
    container_name: \${STACK_NAME}-worker-$i
    hostname: \${STACK_NAME}-worker-$i
    volumes:
      - ./myfiles:/home/myuser/myfiles
      - ./config_files/hadoop/core-site.xml:/home/myuser/hadoop/etc/hadoop/core-site.xml
      - ./config_files/hadoop/hadoop-env.sh:/home/myuser/hadoop/etc/hadoop/hadoop-env.sh
      - ./config_files/hadoop/hdfs-site.xml:/home/myuser/hadoop/etc/hadoop/hdfs-site.xml
      - ./config_files/hadoop/mapred-site.xml:/home/myuser/hadoop/etc/hadoop/mapred-site.xml
      - ./config_files/hadoop/yarn-env.sh:/home/myuser/hadoop/etc/hadoop/yarn-env.sh
      - ./config_files/hadoop/yarn-site.xml:/home/myuser/hadoop/etc/hadoop/yarn-site.xml
      - ./config_files/spark/spark-defaults.conf:/home/myuser/spark/conf/spark-defaults.conf
      - ./config_files/spark/spark-env.sh:/home/myuser/spark/conf/spark-env.sh
      - .env:/home/myuser/.env
    command: ["WORKER"]

EOF
  done

  cat >> "$COMPOSE_FILE" << EOF
  master:
    <<: *spark-common
    container_name: \${STACK_NAME}-master
    hostname: \${STACK_NAME}-master
    build:
      context: .
      dockerfile: Dockerfile
      args:
        SPARK_VERSION: \${SPARK_VERSION}
        HADOOP_VERSION: \${HADOOP_VERSION}
        APT_MIRROR: \${APT_MIRROR}
    ports:
      - "9870:9870/tcp" # HDFS
      - "8088:8088/tcp" # YARN
      - "19888:19888/tcp" # Mapred Job History
      - "18080:18080/tcp" # Spark Job History
      - "15002:15002/tcp" # Spark Connect
      - "8888:8888/tcp" # Jupyter Lab
    volumes:
      - master:/home/myuser/
      - ./myfiles:/home/myuser/myfiles
      - ./config_files/hadoop/core-site.xml:/home/myuser/hadoop/etc/hadoop/core-site.xml
      - ./config_files/hadoop/hadoop-env.sh:/home/myuser/hadoop/etc/hadoop/hadoop-env.sh
      - ./config_files/hadoop/hdfs-site.xml:/home/myuser/hadoop/etc/hadoop/hdfs-site.xml
      - ./config_files/hadoop/mapred-site.xml:/home/myuser/hadoop/etc/hadoop/mapred-site.xml
      - ./config_files/hadoop/yarn-env.sh:/home/myuser/hadoop/etc/hadoop/yarn-env.sh
      - ./config_files/hadoop/yarn-site.xml:/home/myuser/hadoop/etc/hadoop/yarn-site.xml
      - ./config_files/spark/spark-defaults.conf:/home/myuser/spark/conf/spark-defaults.conf
      - ./config_files/spark/spark-env.sh:/home/myuser/spark/conf/spark-env.sh
      - .env:/home/myuser/.env
    command: ["MASTER"]
EOF

  log_info "${LIGHTBLUE_COLOR}docker-compose.yml${RESET_COLORS} file successfully generated for ${YELLOW_COLOR}${NUM_WORKER_NODES}${RESET_COLORS} worker nodes"
  log_info "You may now start the cluster with the following command: ${YELLOW_COLOR}docker compose build && docker compose up${RESET_COLORS}"
fi