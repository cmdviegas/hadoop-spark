#!/bin/sh
# ██████╗  ██████╗ █████╗
# ██╔══██╗██╔════╝██╔══██╗
# ██║  ██║██║     ███████║
# ██║  ██║██║     ██╔══██║
# ██████╔╝╚██████╗██║  ██║
# ╚═════╝  ╚═════╝╚═╝  ╚═╝
# DEPARTAMENTO DE ENGENHARIA DE COMPUTACAO E AUTOMACAO
# UNIVERSIDADE FEDERAL DO RIO GRANDE DO NORTE, NATAL/RN
#
# (C) 2022-2025 CARLOS M D VIEGAS
# https://github.com/cmdviegas
#
# Description: This script dynamically updates the docker-compose.yml file based on the number of worker nodes defined in the .env file.

# You can edit this file to suit your requirements.

# Do not execute this file directly. Instead, use: docker compose run --rm init

# Some color codes for logging
RED_COLOR='\e[0;31m'
GREEN_COLOR='\e[0;32m'
YELLOW_COLOR='\e[0;33m'
LIGHTBLUE_COLOR='\e[0;36m'
RESET_COLORS='\e[0m'
INFO="[${GREEN_COLOR}INFO${RESET_COLORS}]"
ERROR="[${RED_COLOR}ERROR${RESET_COLORS}]"
WARN="[${YELLOW_COLOR}WARN${RESET_COLORS}]"

log_info() { printf "%b %b\n" "${INFO}" "$1"; }
log_warn() { printf "%b %b\n" "${WARN}" "$1"; }
log_error() { printf "%b %b\n" "${ERROR}" "$1"; }

if [ -z "${DOCKER_COMPOSE_RUN}" ]; then
    log_warn "This script must be executed using: ${YELLOW_COLOR}docker compose run --rm init${RESET_COLORS}"
    exit 1
fi

COMPOSE_FILE="docker-compose.yml"
NUM_WORKER_NODES="${NUM_WORKER_NODES}"

generate_compose_file() {
    local num_workers="$1"

    cat > "${COMPOSE_FILE}" << 'EOF'
# ██████╗  ██████╗ █████╗
# ██╔══██╗██╔════╝██╔══██╗
# ██║  ██║██║     ███████║
# ██║  ██║██║     ██╔══██║
# ██████╔╝╚██████╗██║  ██║
# ╚═════╝  ╚═════╝╚═╝  ╚═╝
# DEPARTAMENTO DE ENGENHARIA DE COMPUTACAO E AUTOMACAO
# UNIVERSIDADE FEDERAL DO RIO GRANDE DO NORTE, NATAL/RN
#
# (C) 2022-2025 CARLOS M D VIEGAS
# https://github.com/cmdviegas

# Description: This file sets up a multi-node Hadoop and Spark cluster.

# ⚠️ This file is automatically generated by the init.sh script. 
# You can edit it manually, but any changes will be overwritten the next time init.sh runs.

# If you want to add your own services, use the docker-compose.aux.yml file instead. 
# Then start the cluster with: docker compose -f docker-compose.yml -f docker-compose.aux.yml up

name: ${STACK_NAME}

x-spark-common:
  &spark-common
  image: ${IMAGE_NAME}
  tty: true
  restart: on-failure
  entrypoint: ["bash", "bootstrap.sh"]
  networks:
    - ${STACK_NAME}_network
  secrets:
    - user_password
  environment:
    - MY_SECRETS_FILE=/run/secrets/user_password
    - NUM_WORKER_NODES=${NUM_WORKER_NODES}
    - STACK_NAME=${STACK_NAME}

networks:
  spark_network:
    name: ${STACK_NAME}_network
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.31.0.0/24

volumes:
  master:
    name: ${STACK_NAME}_master_volume
    driver: local

secrets:
  user_password:
    file: .password

services:
  init:
    image: alpine:3.22.0
    container_name: ${STACK_NAME}-init
    profiles: [init]
    tty: true
    restart: no
    working_dir: /workspace
    networks:
      - ${STACK_NAME}_network
    volumes:
      - ./:/workspace
    environment:
      - NUM_WORKER_NODES=${NUM_WORKER_NODES}
      - DOCKER_COMPOSE_RUN=true
      - DOWNLOAD_HADOOP_SPARK=true
    entrypoint: ["sh", "script_files/init.sh"]

EOF

    # Dynamically append worker services based on the number of workers
    i=1
    while [ $i -le "${num_workers}" ]; do
        cat >> "${COMPOSE_FILE}" << EOF
  worker-$i:
    <<: *spark-common
    container_name: \${STACK_NAME}-worker-$i
    hostname: \${STACK_NAME}-worker-$i
    volumes:
      - ./myfiles:/home/myuser/myfiles
      - ./config_files/hadoop/core-site.xml:/home/myuser/hadoop/etc/hadoop/core-site.xml
      - ./config_files/hadoop/hadoop-env.sh:/home/myuser/hadoop/etc/hadoop/hadoop-env.sh
      - ./config_files/hadoop/hdfs-site.xml:/home/myuser/hadoop/etc/hadoop/hdfs-site.xml
      - ./config_files/hadoop/mapred-site.xml:/home/myuser/hadoop/etc/hadoop/mapred-site.xml
      - ./config_files/hadoop/yarn-env.sh:/home/myuser/hadoop/etc/hadoop/yarn-env.sh
      - ./config_files/hadoop/yarn-site.xml:/home/myuser/hadoop/etc/hadoop/yarn-site.xml
      - ./config_files/spark/spark-defaults.conf:/home/myuser/spark/conf/spark-defaults.conf
      - ./config_files/spark/spark-env.sh:/home/myuser/spark/conf/spark-env.sh
      - .env:/home/myuser/.env
    command: ["WORKER"]

EOF
            i=$((i + 1))
        done

        # Append the master service
        cat >> "${COMPOSE_FILE}" << 'EOF'
  master:
    <<: *spark-common
    container_name: ${STACK_NAME}-master
    hostname: ${STACK_NAME}-master
    build:
      context: .
      dockerfile: Dockerfile
      args:
        SPARK_VERSION: ${SPARK_VERSION}
        HADOOP_VERSION: ${HADOOP_VERSION}
        APT_MIRROR: ${APT_MIRROR}
    ports:
      - "9870:9870/tcp" # HDFS
      - "8088:8088/tcp" # YARN
      - "19888:19888/tcp" # Mapred Job History
      - "18080:18080/tcp" # Spark Job History
      - "15002:15002/tcp" # Spark Connect
      - "8888:8888/tcp" # Jupyter Lab
    volumes:
      - master:/home/myuser/
      - ./myfiles:/home/myuser/myfiles
      - ./config_files/hadoop/core-site.xml:/home/myuser/hadoop/etc/hadoop/core-site.xml
      - ./config_files/hadoop/hadoop-env.sh:/home/myuser/hadoop/etc/hadoop/hadoop-env.sh
      - ./config_files/hadoop/hdfs-site.xml:/home/myuser/hadoop/etc/hadoop/hdfs-site.xml
      - ./config_files/hadoop/mapred-site.xml:/home/myuser/hadoop/etc/hadoop/mapred-site.xml
      - ./config_files/hadoop/yarn-env.sh:/home/myuser/hadoop/etc/hadoop/yarn-env.sh
      - ./config_files/hadoop/yarn-site.xml:/home/myuser/hadoop/etc/hadoop/yarn-site.xml
      - ./config_files/spark/spark-defaults.conf:/home/myuser/spark/conf/spark-defaults.conf
      - ./config_files/spark/spark-env.sh:/home/myuser/spark/conf/spark-env.sh
      - .env:/home/myuser/.env
    command: ["MASTER"]
EOF
    # Notify the user of successful file generation
    log_info "${YELLOW_COLOR}docker-compose.yml${RESET_COLORS} file successfully generated for ${YELLOW_COLOR}${num_workers}${RESET_COLORS} worker nodes."
}

# Main script execution starts here
if [ "$1" != "default" ]; then
    # If not 'default', generate compose file with the current number of worker nodes
    generate_compose_file "${NUM_WORKER_NODES}"
else
    # If 'default', reset to 2 workers
    log_info "Reverting ${YELLOW_COLOR}docker-compose.yml${RESET_COLORS} to default state."
    generate_compose_file 2
fi

# Run download.sh if DOWNLOAD_HADOOP_SPARK is true
# It downloads Hadoop and Spark files based on the versions specified in the .env file, and also verifies their checksums
if [ "${DOWNLOAD_HADOOP_SPARK}" = "true" ]; then
    chmod +x ./script_files/download.sh
    if ./script_files/download.sh; then
        log_info "You may now start the cluster with the following command: ${YELLOW_COLOR}docker compose build && docker compose up${RESET_COLORS}"
        exit 0
    else
        log_error "The download.sh script failed. Please check the error above and try again."
        exit 1
    fi
fi