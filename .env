# ██████╗  ██████╗ █████╗
# ██╔══██╗██╔════╝██╔══██╗
# ██║  ██║██║     ███████║
# ██║  ██║██║     ██╔══██║
# ██████╔╝╚██████╗██║  ██║
# ╚═════╝  ╚═════╝╚═╝  ╚═╝
# DEPARTAMENTO DE ENGENHARIA DE COMPUTACAO E AUTOMACAO
# UNIVERSIDADE FEDERAL DO RIO GRANDE DO NORTE, NATAL/RN
#
# (C) 2022-2025 CARLOS M D VIEGAS
# https://github.com/cmdviegas
# 

### The values below should be changed (if needed) before ***BUILD STAGE***

## Hadoop and Spark versions
SPARK_VERSION=3.5.4
HADOOP_VERSION=3.4.1

## System username and password
USERNAME=spark
PASSWORD=spark

## Docker image name
IMAGE_NAME=hadoop-spark:latest

### The values below can be changed (if needed) after the image is built

## Number of worker nodes
REPLICAS=2

## Memory

# MAPREDUCE/HADOOP
MEM_AM=1024 # memory allocated for the ApplicationMaster in MapReduce jobs (yarn.app.mapreduce.am.resource.mb)
MEM_MAP=512 # memory allocated for the map tasks in MapReduce jobs (mapreduce.map.memory.mb)
MEM_RED=512 # memory allocated for the reduce tasks in MapReduce jobs (mapreduce.reduce.memory.mb)

# YARN
MEM_RM=4096 # total memory available to the NodeManager on each node in the cluster (yarn.nodemanager.resource.memory-mb)
MEM_MAX=2048 # maximum memory limit that a single container can request from YARN (yarn.scheduler.maximum-allocation-mb)
# IMPORTANT: yarn.nodemanager.resource.memory-mb >= yarn.scheduler.maximum-allocation-mb (MEM_RM >= MEM_MAX)
MEM_MIN=512 # minimum memory allocation for a container (yarn.scheduler.minimum-allocation-mb)
MAX_SCHED=0.3 # maximum percentage of cluster resources that can be allocated to the ApplicationMaster (yarn.scheduler.capacity.maximum-am-resource-percent)

# SPARK
MEM_DRV=1g # amount of memory to use for the driver process (spark.driver.memory) must be lower than yarn.scheduler.maximum-allocation-mb
MEM_EXE=1g # amount of memory to use for the executor process (spark.executor.memory) must be lower than yarn.scheduler.maximum-allocation-mb
MEM_AMS=1g # amount of memory allocated to the Application Master (AM) in YARN for Spark applications (spark.yarn.am.memory)

# spark.driver.memory + (spark.executor.memory * num_executors) + spark.yarn.am.memory <= yarn.nodemanager.resource.memory-mb