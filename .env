# ██████╗  ██████╗ █████╗
# ██╔══██╗██╔════╝██╔══██╗
# ██║  ██║██║     ███████║
# ██║  ██║██║     ██╔══██║
# ██████╔╝╚██████╗██║  ██║
# ╚═════╝  ╚═════╝╚═╝  ╚═╝
# DEPARTAMENTO DE ENGENHARIA DE COMPUTACAO E AUTOMACAO
# UNIVERSIDADE FEDERAL DO RIO GRANDE DO NORTE, NATAL/RN
#
# (C) 2022-2025 CARLOS M D VIEGAS
# https://github.com/cmdviegas
# 

### The values below can be changed (if needed) before ***BUILD STAGE***

###
##### COMMON
## Hadoop and Spark versions
SPARK_VERSION=3.5.5
HADOOP_VERSION=3.4.1

## System username and password
MY_USERNAME=myuser
MY_PASSWORD=spark

## Spark-connect server? [enable/disable]
SPARK_CONNECT_SERVER=disable
# WARNING: Please be advised that enabling Spark Connect will prevent you from running the PySpark terminal

## Docker image name
IMAGE_NAME=spark:latest
STACK_NAME=spark

## Ubuntu apt mirror
## Optional: Set a custom Ubuntu APT mirror (e.g., a local or regional mirror)
## Leave this variable blank to use the default mirror (http://archive.ubuntu.com/ubuntu)
APT_MIRROR=

### The values below can be changed (if needed) anytime (i.e. even after the image is built)

## Number of worker nodes
NUM_WORKER_NODES=2

###
##### MAPREDUCE/HADOOP memory resources
YARN_APP_MAPREDUCE_AM_RESOURCE_MB=1024
# Memory allocated for the ApplicationMaster in MapReduce jobs (yarn.app.mapreduce.am.resource.mb)
MAPREDUCE_MAP_MEMORY_MB=512
# Memory allocated for the map tasks in MapReduce jobs (mapreduce.map.memory.mb)
MAPREDUCE_REDUCE_MEMORY_MB=512
# Memory allocated for the reduce tasks in MapReduce jobs (mapreduce.reduce.memory.mb)

###
##### YARN memory resources
YARN_NODEMANAGER_RESOURCE_MEMORY_MB=2048
# Total memory available to the NodeManager on each node in the cluster (yarn.nodemanager.resource.memory-mb)
YARN_SCHEDULER_MAXIMUM_ALLOCATION_MB=1536
# Maximum memory limit that a single container can request from YARN (yarn.scheduler.maximum-allocation-mb)
# IMPORTANT: yarn.scheduler.maximum-allocation-mb <= yarn.nodemanager.resource.memory-mb
YARN_SCHEDULER_MINIMUM_ALLOCATION_MB=512
# Minimum memory allocation for a container (yarn.scheduler.minimum-allocation-mb)
YARN_SCHEDULER_CAPACITY_MAXIMUM_AM_RESOURCE_PERCENT=0.4
# Maximum percentage of cluster resources that can be allocated to the ApplicationMaster (yarn.scheduler.capacity.maximum-am-resource-percent)

###
##### SPARK memory resources
SPARK_DRIVER_MEMORY=1g
# Amount of memory to use for the driver process (spark.driver.memory) must be lower than yarn.scheduler.maximum-allocation-mb
SPARK_EXECUTOR_MEMORY=1g
# Amount of memory to use for the executor process (spark.executor.memory) must be lower than yarn.scheduler.maximum-allocation-mb
SPARK_YARN_AM_MEMORY=512m
# Amount of memory allocated to the Application Master (AM) in YARN for Spark applications (spark.yarn.am.memory)
# IMPORTANT: spark.driver.memory + (spark.executor.memory * spark.executor.instances) + spark.yarn.am.memory <= yarn.nodemanager.resource.memory-mb

# IMPORTANT 2:
# mapreduce.map.memory.mb <= yarn.scheduler.maximum-allocation-mb
# mapreduce.reduce.memory.mb <= yarn.scheduler.maximum-allocation-mb
# spark.driver.memory <= yarn.scheduler.maximum-allocation-mb
# spark.executor.memory <= yarn.scheduler.maximum-allocation-mb
# spark.yarn.am.memory <= yarn.scheduler.maximum-allocation-mb
