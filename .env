# ██████╗  ██████╗ █████╗
# ██╔══██╗██╔════╝██╔══██╗
# ██║  ██║██║     ███████║
# ██║  ██║██║     ██╔══██║
# ██████╔╝╚██████╗██║  ██║
# ╚═════╝  ╚═════╝╚═╝  ╚═╝
# DEPARTAMENTO DE ENGENHARIA DE COMPUTACAO E AUTOMACAO
# UNIVERSIDADE FEDERAL DO RIO GRANDE DO NORTE, NATAL/RN
#
# (C) 2022-2025 CARLOS M D VIEGAS
# https://github.com/cmdviegas
 
# Description: This file configures key parameters for building and running a Hadoop and Spark cluster in a Docker environment

# Hadoop and Spark versions
SPARK_VERSION=3.5.6
HADOOP_VERSION=3.4.1

# Spark-connect server? [enable/disable]
SPARK_CONNECT_SERVER=disable
# WARNING: Please be advised that enabling Spark Connect will prevent local PySpark terminal and JupyterLab usage

# Docker image name
IMAGE_NAME=spark:latest
STACK_NAME=spark

# Ubuntu apt mirror
# Optional: Set a custom Ubuntu APT mirror (e.g., a local or regional mirror)
# Leave this variable blank to use the default mirror (http://archive.ubuntu.com/ubuntu)
#APT_MIRROR=http://br.archive.ubuntu.com/ubuntu
APT_MIRROR=

# Number of worker nodes
NUM_WORKER_NODES=2




# If you need to configure resources for Hadoop or Spark, you can directly edit the properties in the config_files/ folder. Afterwards, restart the cluster for the changes to take effect across all nodes.

##
### FILE: config_files/hadoop/mapred-site.xml:
## yarn.app.mapreduce.am.resource.memory-mb
## yarn.app.mapreduce.am.resource.vcores
## mapreduce.map.resource.memory-mb
## mapreduce.reduce.resource.memory-mb
## mapreduce.map.resource.vcores
## mapreduce.reduce.resource.vcores

##
### FILE: config_files/hadoop/yarn-site.xml:
## yarn.nodemanager.resource.memory-mb
## yarn.scheduler.maximum-allocation-mb
## yarn.scheduler.minimum-allocation-mb
## yarn.scheduler.capacity.maximum-am-resource-percent

# IMPORTANT
# mapreduce.map.resource.memory-mb <= yarn.scheduler.maximum-allocation-mb
# mapreduce.reduce.resource.memory-mb <= yarn.scheduler.maximum-allocation-mb

##
### FILE: config_files/spark/spark-defaults.conf
## spark.driver.memory
## spark.executor.memory
## spark.yarn.am.memory
## spark.executor.pyspark.memory
## spark.executor.memoryOverhead
## spark.executor.cores

# IMPORTANT
# spark.driver.memory <= yarn.scheduler.maximum-allocation-mb
# spark.executor.memory <= yarn.scheduler.maximum-allocation-mb
# spark.yarn.am.memory <= yarn.scheduler.maximum-allocation-mb
